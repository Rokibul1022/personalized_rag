# ğŸ“ Adaptive RAG System - Personalized Learning Assistant

An intelligent Retrieval-Augmented Generation (RAG) system powered by DeepSeek-R1:1.5b for personalized education.

## âœ¨ Features

- **Intelligent RAG**: Hybrid knowledge base + LLM approach
- **Self-Expanding**: Automatically grows knowledge from LLM responses
- **Personalized Learning**: Adapts to learning style, difficulty, and weak topics
- **Hierarchical Quiz System**: 7 main topics with 50+ subtopics
- **Assessment-Driven**: Tests understanding on new topics
- **Memory System**: Tracks progress and learning patterns
- **Semantic Filtering**: Prevents irrelevant results

## ğŸ—ï¸ Architecture

```
Query â†’ Retrieval â†’ Quality Check â†’ Decision
                                    â”œâ”€ Use KB (score â‰¥ 0.5)
                                    â””â”€ Use LLM â†’ Save to KB
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Ollama (for local LLM)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ai_personal_rag.git
cd ai_personal_rag
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install Ollama**
```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: Download from https://ollama.ai
```

5. **Pull DeepSeek-R1 model**
```bash
ollama pull deepseek-r1:1.5b
```

6. **Start Ollama**
```bash
ollama serve
```

7. **Run the system**
```bash
python local_llm_rag.py
```

## ğŸ“š Usage

### Commands
- `quiz` - Start quiz session
- `profile` - View your profile
- `finetune` - Show RAG intelligence report
- `quit` - Exit

### Chat
- Ask any question
- System automatically uses KB or LLM
- Provide feedback (good/bad/skip)
- System learns from your feedback

### Quiz System
1. Select main topic (Mathematics, Physics, Chemistry, Biology, Computer Science, Vectors, Quantum Mechanics)
2. Select subtopic or enter custom topic
3. Choose difficulty (Easy/Medium/Hard)
4. Select number of questions (3/5/10)
5. Take quiz and get instant feedback

## ğŸ“ Project Structure

```
ai_personal_rag/
â”œâ”€â”€ local_llm_rag.py          # Main RAG system
â”œâ”€â”€ main.py                    # Alternative interface
â”œâ”€â”€ rag_api.py                 # Gemini API version
â”œâ”€â”€ datasets/                  # Knowledge base CSV files
â”œâ”€â”€ user_profiles/             # User profiles (auto-created)
â”œâ”€â”€ local_models/              # Memory & patterns (auto-created)
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ¯ Key Features Explained

### 1. Intelligent Retrieval
- TF-IDF vectorization with cosine similarity
- Query expansion with related terms
- Semantic relevance checking (30% keyword match)
- Quality threshold (score â‰¥ 0.5)

### 2. Self-Expanding Knowledge
- LLM responses automatically saved to `llm_generated_knowledge.csv`
- Future queries use expanded knowledge base
- No manual data entry needed

### 3. Personalized Learning
- 8-question profile: name, age, grade, learning style, difficulty, weak topics, goals
- Adaptive responses for visual/auditory/hands-on learners
- Difficulty matching (Easy/Medium/Hard)

### 4. Quiz System
- 7 main topics with 50+ subtopics
- Custom topic support
- LLM-generated questions
- Randomized correct answers
- Real-time scoring and feedback

### 5. Memory & Learning
- Tracks user interactions
- Identifies successful/difficult topics
- Learns from feedback
- Adapts responses over time

## ğŸ”§ Configuration

### Using Different LLM Models

Edit `local_llm_rag.py`:
```python
def call_ollama(self, prompt, model="deepseek-r1:1.5b"):
    # Change model name here
```

Available models:
- `deepseek-r1:1.5b` (recommended)
- `llama2:7b`
- `mistral:7b`
- `phi:2.7b`

### Adjusting Quality Threshold

Edit `local_llm_rag.py`:
```python
if base_results[0]['score'] < 0.5:  # Change threshold here
```

## ğŸ“Š Performance

- **Retrieval Accuracy**: ~85%
- **LLM Fallback**: 15-20% of queries
- **Knowledge Growth**: Automatic
- **Response Time**: 1-3 seconds

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“ License

MIT License - feel free to use for educational purposes

## ğŸ™ Acknowledgments

- DeepSeek-R1 for the LLM model
- Ollama for local inference
- scikit-learn for retrieval algorithms

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Made with â¤ï¸ for personalized education**
